{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sR2Bpn64vQ1H","executionInfo":{"status":"ok","timestamp":1740438321355,"user_tz":480,"elapsed":21309,"user":{"displayName":"דניאל כהן","userId":"09843425876213038656"}},"outputId":"b2525a97-c244-438f-e7a2-d1fbfc32e46c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["A path to a custom database made for this project using the create_database.ipynb we made"],"metadata":{"id":"uEnWP6Dcysel"}},{"cell_type":"code","source":["database_path = '/content/drive/MyDrive/Discord_Manager_files/discord_commands.jsonl'"],"metadata":{"id":"ET5_RH4XIemk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Relevant imports:"],"metadata":{"id":"TaK0eoWW9KFr"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import json\n","from transformers import BertModel, BertTokenizer, GPT2Model, GPT2Tokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"cuTrhgPXw7cu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implementing BERT model for multilabel classifications:\n","in this case we must classify the input action, username and role.\n","while all of the commands the bot will support require action and username, some commands wont use \"role\" and therefore will be\n","\n","classified as < ROLE_100 >"],"metadata":{"id":"GuqdiO9Awzg_"}},{"cell_type":"code","source":["class BertForMultiLabel(nn.Module):\n","    def __init__(self, num_actions, num_users, num_roles):\n","        super(BertForMultiLabel, self).__init__()\n","        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # Classification heads\n","        self.action_classifier = nn.Linear(self.bert.config.hidden_size, num_actions)\n","        self.user_classifier = nn.Linear(self.bert.config.hidden_size, num_users)\n","        self.role_classifier = nn.Linear(self.bert.config.hidden_size, num_roles)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","\n","        action_result = self.action_classifier(pooled_output)\n","        user_result = self.user_classifier(pooled_output)\n","        role_result = self.role_classifier(pooled_output)\n","\n","        return action_result, user_result, role_result"],"metadata":{"id":"NzNF-DFMxKMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GPT2ForMultiLabel(nn.Module):\n","    def __init__(self, num_actions, num_users, num_roles):\n","        super(GPT2ForMultiLabel, self).__init__()\n","        self.gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # Classification heads\n","        self.action_classifier = nn.Linear(self.gpt2.config.hidden_size, num_actions)\n","        self.user_classifier = nn.Linear(self.gpt2.config.hidden_size, num_users)\n","        self.role_classifier = nn.Linear(self.gpt2.config.hidden_size, num_roles)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n","\n","        # Get the last token representation\n","        last_hidden_state = outputs.last_hidden_state\n","        pooled_output = last_hidden_state[:, -1, :]  # Use the last token's hidden state\n","\n","        action_result = self.action_classifier(pooled_output)\n","        user_result = self.user_classifier(pooled_output)\n","        role_result = self.role_classifier(pooled_output)\n","\n","        return action_result, user_result, role_result\n","\n"],"metadata":{"id":"qfOvN5aayPNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A class made to convert the dataset into a tokenized version of input and output so the model could handle it."],"metadata":{"id":"gQfwxq9dxU9v"}},{"cell_type":"code","source":["class DiscordDataset(Dataset):\n","    def __init__(self, data, action_map, user_map, role_map, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.action_map = action_map\n","        self.user_map = user_map\n","        self.role_map = role_map\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx]\n","        input_text = sample[\"input\"]\n","        action = self.action_map[sample[\"output\"][\"action\"]]\n","        user = self.user_map[sample[\"output\"][\"user\"]]\n","        if \"role\" in sample[\"output\"]: # Some actions may not have roles\n","            role = self.role_map[sample[\"output\"][\"role\"]]\n","        else:\n","            role = self.role_map[\"<ROLE_100>\"]\n","\n","        encoding = self.tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n","\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n","            \"action\": torch.tensor(action, dtype=torch.long),\n","            \"user\": torch.tensor(user, dtype=torch.long),\n","            \"role\": torch.tensor(role, dtype=torch.long)\n","        }"],"metadata":{"id":"e1oUi4G7IK93"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Defining user, role and action maps"],"metadata":{"id":"SHlk7iROyK_v"}},{"cell_type":"code","source":["user_index_map = {f\"<USER_{i+1}>\": i for i in range(100)}\n","role_index_map = {f\"<ROLE_{i+1}>\": i for i in range(100)}\n","action_map = {\"ban\": 0, \"add_role\": 1, \"remove_role\": 2}"],"metadata":{"id":"IFIXyOrcIsuk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We train the model, first by using the dataset that we specifically created for this task."],"metadata":{"id":"A1mafJfwyd_W"}},{"cell_type":"code","source":["raw_dataset = []\n","with open(database_path, \"r\") as file:\n","  for line in file:\n","    line = line.strip()\n","    raw_dataset.append(json.loads(line))"],"metadata":{"id":"kwvq0r7hLYLt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Splitting the dataset to train, test and validation."],"metadata":{"id":"HOa-eowkyy7V"}},{"cell_type":"markdown","source":["Training the model and printing its progress: (training bert, and GPT2)"],"metadata":{"id":"xj7Zu-mpzJI1"}},{"cell_type":"code","source":["should_run = False      #if you dont have a version of the trained model on your drive change this to True (retrain the model)\n","if should_run:\n","  tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","  train_data, val_data = train_test_split(raw_dataset, test_size=0.1, random_state=42) # :)\n","  train_dataset = DiscordDataset(train_data, action_map, user_index_map, role_index_map, tokenizer)\n","  val_dataset = DiscordDataset(val_data, action_map, user_index_map, role_index_map, tokenizer)\n","  train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","  val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","\n","\n","  model = BertForMultiLabel(num_actions=len(action_map), num_users=len(user_index_map), num_roles=len(role_index_map))\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model.to(device)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n","  num_epochs = 6\n","  for epoch in range(num_epochs):\n","      model.train()\n","      total_train_loss = 0\n","\n","      for batch in train_loader:\n","          input_ids = batch[\"input_ids\"].to(device)\n","          attention_mask = batch[\"attention_mask\"].to(device)\n","          action_labels = batch[\"action\"].to(device)\n","          user_labels = batch[\"user\"].to(device)\n","          role_labels = batch[\"role\"].to(device)\n","\n","          optimizer.zero_grad()\n","          action_logits, user_logits, role_logits = model(input_ids, attention_mask)\n","\n","          loss = (\n","              criterion(action_logits, action_labels) +\n","              criterion(user_logits, user_labels) +\n","              criterion(role_logits, role_labels)\n","          )\n","\n","          loss.backward()\n","          optimizer.step()\n","          total_train_loss += loss.item()\n","\n","      avg_train_loss = total_train_loss / len(train_loader)\n","\n","      # Validation Step\n","      model.eval()\n","      total_val_loss = 0\n","      correct_actions, correct_users, correct_roles = 0, 0, 0\n","      total_samples = 0\n","\n","      with torch.no_grad():\n","          for batch in val_loader:\n","              input_ids = batch[\"input_ids\"].to(device)\n","              attention_mask = batch[\"attention_mask\"].to(device)\n","              action_labels = batch[\"action\"].to(device)\n","              user_labels = batch[\"user\"].to(device)\n","              role_labels = batch[\"role\"].to(device)\n","\n","              action_logits, user_logits, role_logits = model(input_ids, attention_mask)\n","\n","              loss = (\n","                  criterion(action_logits, action_labels) +\n","                  criterion(user_logits, user_labels) +\n","                  criterion(role_logits, role_labels)\n","              )\n","              total_val_loss += loss.item()\n","\n","              # Compute Accuracy\n","              correct_actions += (torch.argmax(action_logits, dim=1) == action_labels).sum().item()\n","              correct_users += (torch.argmax(user_logits, dim=1) == user_labels).sum().item()\n","              correct_roles += (torch.argmax(role_logits, dim=1) == role_labels).sum().item()\n","              total_samples += input_ids.size(0)\n","\n","      avg_val_loss = total_val_loss / len(val_loader)\n","      action_acc = correct_actions / total_samples\n","      user_acc = correct_users / total_samples\n","      role_acc = correct_roles / total_samples\n","\n","      print(f\"Epoch {epoch+1}/{num_epochs}\")\n","      print(f\"Train Loss: {avg_train_loss:.4f}\")\n","      print(f\"Val Loss: {avg_val_loss:.4f}\")\n","      print(f\"Action Acc: {action_acc:.4f}, User Acc: {user_acc:.4f}, Role Acc: {role_acc:.4f}\")\n","\n","  # Save the trained model\n","  torch.save(model.state_dict(), '/content/drive/MyDrive/Discord_Manager_files/bert_discord_model.pth')"],"metadata":{"id":"mRFBhWShMV7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["should_run = False     #if you dont have a version of the trained model on your drive change this to True (retrain the model)\n","if should_run:\n","  tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","  tokenizer.pad_token = tokenizer.eos_token\n","\n","  train_data, val_data = train_test_split(raw_dataset, test_size=0.1, random_state=42) # :)\n","  train_dataset = DiscordDataset(train_data, action_map, user_index_map, role_index_map, tokenizer)\n","  val_dataset = DiscordDataset(val_data, action_map, user_index_map, role_index_map, tokenizer)\n","  train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","  val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","\n","  model = GPT2ForMultiLabel(num_actions=len(action_map), num_users=len(user_index_map), num_roles=len(role_index_map))\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model.to(device)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n","  num_epochs = 6\n","  for epoch in range(num_epochs):\n","      model.train()\n","      total_train_loss = 0\n","\n","      for batch in train_loader:\n","          input_ids = batch[\"input_ids\"].to(device)\n","          attention_mask = batch[\"attention_mask\"].to(device)\n","          action_labels = batch[\"action\"].to(device)\n","          user_labels = batch[\"user\"].to(device)\n","          role_labels = batch[\"role\"].to(device)\n","\n","          optimizer.zero_grad()\n","          action_logits, user_logits, role_logits = model(input_ids, attention_mask)\n","\n","          loss = (\n","              criterion(action_logits, action_labels) +\n","              criterion(user_logits, user_labels) +\n","              criterion(role_logits, role_labels)\n","          )\n","\n","          loss.backward()\n","          optimizer.step()\n","          total_train_loss += loss.item()\n","\n","      avg_train_loss = total_train_loss / len(train_loader)\n","\n","      # Validation Step\n","      model.eval()\n","      total_val_loss = 0\n","      correct_actions, correct_users, correct_roles = 0, 0, 0\n","      total_samples = 0\n","\n","      with torch.no_grad():\n","          for batch in val_loader:\n","              input_ids = batch[\"input_ids\"].to(device)\n","              attention_mask = batch[\"attention_mask\"].to(device)\n","              action_labels = batch[\"action\"].to(device)\n","              user_labels = batch[\"user\"].to(device)\n","              role_labels = batch[\"role\"].to(device)\n","\n","              action_logits, user_logits, role_logits = model(input_ids, attention_mask)\n","\n","              loss = (\n","                  criterion(action_logits, action_labels) +\n","                  criterion(user_logits, user_labels) +\n","                  criterion(role_logits, role_labels)\n","              )\n","              total_val_loss += loss.item()\n","\n","              # Compute Accuracy\n","              correct_actions += (torch.argmax(action_logits, dim=1) == action_labels).sum().item()\n","              correct_users += (torch.argmax(user_logits, dim=1) == user_labels).sum().item()\n","              correct_roles += (torch.argmax(role_logits, dim=1) == role_labels).sum().item()\n","              total_samples += input_ids.size(0)\n","\n","      avg_val_loss = total_val_loss / len(val_loader)\n","      action_acc = correct_actions / total_samples\n","      user_acc = correct_users / total_samples\n","      role_acc = correct_roles / total_samples\n","\n","      print(f\"Epoch {epoch+1}/{num_epochs}\")\n","      print(f\"Train Loss: {avg_train_loss:.4f}\")\n","      print(f\"Val Loss: {avg_val_loss:.4f}\")\n","      print(f\"Action Acc: {action_acc:.4f}, User Acc: {user_acc:.4f}, Role Acc: {role_acc:.4f}\")\n","\n","  # Save the trained model\n","  torch.save(model.state_dict(), '/content/drive/MyDrive/Discord_Manager_files/gpt2_discord_model.pth')"],"metadata":{"id":"D7tti4mVyTa-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prediction phase- the model's output.\n","can be used to predict single sample outputs\n","(the main function of prediction we use in responses)"],"metadata":{"id":"6sJ3Dyq0zQCE"}},{"cell_type":"code","source":["def predict(model, tokenizer, text):\n","    model.eval()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    encoding = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n","    input_ids = encoding[\"input_ids\"].to(device)\n","    attention_mask = encoding[\"attention_mask\"].to(device)\n","\n","    with torch.no_grad():\n","        action_logits, user_logits, role_logits = model(input_ids, attention_mask)\n","\n","    action = torch.argmax(action_logits, dim=1).item()\n","    user = torch.argmax(user_logits, dim=1).item()\n","    role = torch.argmax(role_logits, dim=1).item()\n","\n","    action_name = [k for k, v in action_map.items() if v == action][0]\n","    user_name = f\"<USER_{user+1}>\"  # Convert index to placeholder\n","    role_name = f\"<ROLE_{role+1}>\"\n","    return {\"action\": action_name, \"user\": user_name, \"role\": role_name}\n"],"metadata":{"id":"tHwbGZbmShTy"},"execution_count":null,"outputs":[]}]}